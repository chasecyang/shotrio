# AI 助手功能测试指南

## 测试场景

### 1. 基础单轮对话测试

**测试目标**：验证 AI 能理解简单请求并执行单个操作

#### 测试用例 1.1：查询剧本内容
- 用户输入："查看当前剧集的剧本"
- 期望结果：
  - AI 调用 `query_script_content` 工具
  - 显示剧本内容摘要
  - 有 thinking process（思考过程）显示

#### 测试用例 1.2：查询素材库
- 用户输入："看看项目里有哪些角色"
- 期望结果：
  - AI 调用 `query_assets` 工具，带参数 `tags: "character"`
  - 返回角色列表
  - 显示数量统计

#### 测试用例 1.3：需要确认的操作
- 用户输入："从剧本中提取道具"
- 期望结果：
  - AI 解释将要执行的操作
  - 出现"待确认操作"卡片
  - 需要用户点击确认才执行

---

### 2. 多轮自主执行测试（Agent Loop）

**测试目标**：验证 AI 能够连续执行多个只读操作，自主完成复杂任务

#### 测试用例 2.1：查询并分析
- 用户输入："帮我分析一下当前剧集的分镜情况"
- 期望行为：
  1. AI 先调用 `query_shots` 查询分镜列表
  2. 基于查询结果，AI 分析并总结
  3. 可能进一步调用 `analyze_project_stats` 获取统计信息
  4. 最后给出完整的分析报告

期望结果：
- 看到多个"正在执行"提示
- 每个工具执行结果都反馈给 AI
- AI 能基于结果继续决策
- 最终给出综合性回复

#### 测试用例 2.2：条件判断执行
- 用户输入："帮我生成所有还没有图片的分镜的图片"
- 期望行为：
  1. AI 调用 `query_shots` 查询所有分镜
  2. AI 分析哪些分镜没有图片
  3. AI 调用 `generate_shot_images`（需要确认）
  4. 出现待确认操作

期望结果：
- 可以看到 AI 的推理过程："发现有 X 个分镜还没有图片"
- 待确认操作显示具体的分镜 ID 列表

---

### 3. 流式响应测试

**测试目标**：验证流式输出是否流畅，用户体验是否良好

#### 测试用例 3.1：观察流式输出
- 用户输入：任意请求
- 观察点：
  - 是否看到"🤔 正在思考..."状态
  - AI 的回复是否逐步显示（而不是一次性出现）
  - 工具执行时是否有实时提示
  - 是否有卡顿或延迟

#### 测试用例 3.2：thinking process 显示
- 任意请求后，点击"思考过程"按钮
- 期望结果：
  - 能看到 AI 的 reasoning content
  - 内容是 AI 的推理过程，不是随机文本
  - 折叠/展开功能正常

---

### 4. 复杂任务测试

**测试目标**：验证 AI 能处理需要多步骤的复杂任务

#### 测试用例 4.1：完整工作流
- 场景：用户想创建一个新的剧集并生成分镜
- 用户输入："帮我查看当前剧集的剧本，然后生成分镜"
- 期望行为：
  1. AI 先查询剧本内容
  2. 基于剧本内容，AI 建议生成分镜
  3. 调用 `generate_storyboard`（需要确认）

#### 测试用例 4.2：错误处理
- 场景：请求一个不存在的资源
- 用户输入："查询 ID 为 'xxx' 的分镜"
- 期望结果：
  - AI 调用工具但执行失败
  - AI 能理解错误信息并向用户解释
  - 建议用户采取其他操作

#### 测试用例 4.3：中断处理
- 场景：在 Agent Loop 中遇到需要确认的操作
- 用户输入："帮我查询所有分镜，然后删除没有图片的分镜"
- 期望行为：
  1. AI 查询分镜
  2. AI 发现需要删除操作
  3. 暂停并请求用户确认
  4. 用户可以选择确认或取消

---

### 5. Reasoning 模型测试

**测试目标**：验证 DeepSeek reasoner 模型的 thinking 能力

#### 测试用例 5.1：复杂推理
- 用户输入："我想让这个剧集的节奏更快，应该怎么做？"
- 期望结果：
  - thinking process 中能看到 AI 的分析：
    - 分析当前分镜的时长
    - 考虑拆解分镜或缩短时长
    - 给出具体建议
  - AI 的回复是基于推理过程的

#### 测试用例 5.2：多方案对比
- 用户输入："我想生成一些科技感装备，你有什么建议？"
- 期望结果：
  - thinking process 显示 AI 考虑了多种设计方案
  - 最终回复中提供 2-3 个具体方案

---

## 测试检查清单

### 功能完整性
- [ ] AI 能理解自然语言请求
- [ ] 能正确调用工具（function calling）
- [ ] 只读操作自动执行
- [ ] 写操作需要用户确认
- [ ] Agent Loop 能连续执行多个工具
- [ ] 遇到需要确认的操作时正确暂停

### 用户体验
- [ ] 流式输出流畅，无明显卡顿
- [ ] 有清晰的状态提示（思考中、执行中等）
- [ ] thinking process 可以查看
- [ ] 错误信息清晰易懂
- [ ] 响应速度合理（一般 < 10 秒）

### 稳定性
- [ ] 不会无限循环（有最大迭代次数限制）
- [ ] 错误能正确捕获和处理
- [ ] 网络中断能恢复
- [ ] 不会丢失对话历史

---

## 已知限制

1. **最大迭代次数**：Agent Loop 最多执行 5 轮，防止无限循环
2. **Context 限制**：历史消息保留最近 50 条
3. **流式响应**：某些复杂操作可能需要等待完整响应
4. **Function Calling**：一次只能调用一个工具，不支持并行调用

---

## 性能指标

- **首次响应时间**：< 3 秒（第一个 token）
- **完整响应时间**：< 10 秒（简单查询）
- **复杂任务时间**：< 30 秒（包含多次工具调用）
- **流式输出延迟**：< 100ms（chunk 间隔）

---

## 调试技巧

### 查看控制台日志
```
[Agent Chat] 发送请求到 AI，functions: 25
[Agent Chat] AI 响应: { hasContent: true, hasFunctionCall: true, hasReasoning: true }
[Agent Loop] 第 1 轮，消息数：3
[Agent Loop] 执行只读操作: query_shots
```

### 检查网络请求
- 打开浏览器开发者工具 -> Network
- 查找 `/api/agent/chat-stream` 请求
- 查看响应流的内容

### 验证 thinking process
- 点击消息下方的"思考过程"按钮
- 查看 AI 的推理过程是否合理
- 确认 reasoning 内容不为空

---

## 下一步优化方向

1. **并行工具调用**：支持同时调用多个只读工具
2. **更智能的上下文管理**：自动压缩长对话历史
3. **工具调用追踪**：显示每个工具的详细执行日志
4. **用户反馈循环**：允许用户对 AI 的回复进行评价
5. **预设任务模板**：常用操作的快捷入口

---

## 测试完成标准

✅ 所有测试用例通过
✅ 用户体验流畅
✅ 没有严重 bug
✅ 性能指标达标
✅ 文档完善

---

**测试人员**：_________  
**测试日期**：_________  
**测试版本**：v1.0.0  
**测试结果**：通过 / 未通过

